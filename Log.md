
## 100 Days Of Machine Learning - LOG
#### Two reasons why I am going to log/document my everyday progress here is, 
1. To keep track of my work
2. To prepare a document sequentially based on my learning, so that It will help to other newbie's.

### Day 1: August 26, 2018

**Today's Progress** :
* Basic understanding of Machine learning - source: Andrew Ng course from Coursera
* Types Machine learning
	* Supervised learning - The main goal in supervised learning is to learn a model from labeled training data that allows us to make predictions about unseen or future data.
	* Unsupervised learning - Here we deal with un-labelled data of unknown structure. This technique allows pile of information into meaningful subgroups (clusters) without having any prior knowledge of their group memberships.
	* Reinforcement learning - In reinforcement learning, the goal is to develop a system (agent) that improves its performance based on interactions with the environment 

* Importing libraries
* Reading datasets using pandas and numpy libraries
* Read about Data Preprocessing
    * Handling missing data in dataset
    * Handling Categorical variables with OneHotEncoders
 Since Machine Learning models are based on mathematical equations, so it can cause problems with this kind of variables.

#### I will do some practicals tomorrow based on today readings and upload here.

### Day 2: August 27, 2018

**Today's Progress** :
* Practiced handling missing data - [Link for code](https://github.com/reddimohan/100-Days-of-ML-Code/blob/master/Day_02_Data_Preprocessing_practice/data_preprocessing.ipynb)
* Practiced handling categorical variables using OneHotEncoder from sklearn - [Link for code](https://github.com/reddimohan/100-Days-of-ML-Code/blob/master/Day_02_Data_Preprocessing_practice/categorical_variables_one-hot-encoding.ipynb)


### Day 3: August 28, 2018

**Today's Progress** :
* Splitting dataset into train and test - [Link for code](https://github.com/reddimohan/100-Days-of-ML-Code/blob/master/Day_03_split_dataset_into_train_test/train_test_split.ipynb)
* Feature scaling


### Day 4: August 29, 2018

**Today's Progress** :
* Started reading about Linear Regression


### Day 5: August 30, 2018

**Today's Progress** :
* Continued reading about Linear Regression


### Day 6: August 31, 2018

**Today's Progress** :
* Completed reading about Linear algebra toipc - Andrew Ng [course link](https://www.youtube.com/playlist?list=PLnnr1O8OWc6boN4WHeuisJWmeQHH9D_Vg)


### Day 7: Sept 1, 2018

**Today's Progress** :
* Completed Week 1 from Andrew Ng course from Coursera  - [Week 1](https://www.coursera.org/learn/machine-learning/home/week/1)


### Day 8: Sept 2, 2018

**Today's Progress** :
* Read about Introduction to Computer vision


### Day 9: Sept 3, 2018

**Today's Progress** :
* Read about,
    * Viola & Jones Algorithm
    * Haar like features
    * Integral Image


### Day 10: Sept 4, 2018

**Today's Progress** :
* Read about,
    * Started Installing openCV from http://www.codebind.com/linux-tutorials/install-opencv-ubuntu-18-04-lts-python/, all steps are done, and still not able to import it. Will debug this tomorrow.


### Day 11: Sept 5, 2018

**Today's Progress** :
* Read about Training Classifier
* Read about Adaboost boosting algorithm


### Day 12: Sept 6, 2018

**Today's Progress** :
* Worked on Basic face and smile detection using opencv, But It is not very accurate. [Here is the code](https://github.com/reddimohan/100-Days-of-ML-Code/blob/master/Day_12_Basic_face_detection/face_detection.ipynb)

**Note:** Does anybody know, which parameter I should be changing to increase the accuracy.


### Day 13: Sept 7, 2018

**Today's Progress** :
* Read about SSD (Single Shot Multibox Detector) Algorithm and Predicting Object position. 



### Day 14: Sept 8, 2018

**Today's Progress** :
* Read about Object Detection with SSD


### Day 15: Sept 9, 2018

**Today's Progress** :
* Read about Gradient descent
Gradient descent is a optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function.
* Read about back propagation


### Day 16: Sept 10, 2018

**Today's Progress** :
* Read about *Cross Validation*
-- We do *Cross validation* after training the model to test the *error rate*. If *error rate* is low it means we have builded a good model, If *error rate* is high then model is not good.
--  This is one method where we can check the performance of the model
-- It helps in selecting the *best fit model*
-- It ensures that *model is not over fit*
* Types of *Cross Validation*
	* Hold Out Method
	* K-Fold CV
	* Leave One out CV
	* Bootstrap method
* 
k - fold cross validation
variance and bias trade off

### Missed somedays between due to medical emergency

### Day 17: Sept 23, 2018
* Started with Titanic survival prediction challenge in Kaggle



### Day 18: Sept 24, 2018
* Read about KNeighborsClassifier
* Continued working on Kaggle Titanic dataset [Code](https://github.com/reddimohan/100-Days-of-ML-Code/blob/master/Day_17_to_%3F%3F_Kaggle_Titanic:%20Machine%20Learning%20from%20Disaster/Titanic%20survival%20prediction.ipynb)

### Day 19 to 25 : Sept 25 - 31, 2018
* Learned about productionizing Machine learning models with Flask, Gunicorn and Nginx

### Day 26 and 27 : Oct 1 & 2, 2018
* Completed Titanic Kaggle challenge with 78% Model accuracy

### Day 27 and 29 : Oct 3 & 5, 2018
* Read about Feature Engineering
